{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSINESS UNDERSTANDING\n",
    "\n",
    "Overview\n",
    "Kenya Power and Lighting Company (KPLC) often receives a high volume of tweets from customers reporting issues, asking questions, or providing feedback.Understanding customer sentiment towards KPLC is crucial to enable automating of responses, enhancing customer service efficiency, improving response times, and reduce the manual workload on customer service teams. The goal is to develop a chatbot capable of classifying various types of tweets and generating appropriate automated responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "KPLC needs an automated sentiment analysis system to process and categorize customer feedback from social media, particularly X formerly (Twitter) where customers frequently express their sentiments regarding KPLC's services. By accurately classifying tweets related to KPLC‚Äôs services into sentiment categories the system will be able to identify issues by pinpointing common complaints and service issues and enhance customer feedback\n",
    "\n",
    "### Objectives\n",
    "\n",
    "* To gauge overall customer sentiment towards KPLC's services.¬∑   \n",
    "\n",
    "* To Identify specific issues mentioned in the tweets, such as token problems, power outages, billing issues, etc.\n",
    "\n",
    "* To Create a chatbot that provides appropriate responses to customer inquiries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "1. Data Collection and Preprocessing:\n",
    "Gathering relevant tweets mentioning KPLC, especially when customers use various hashtags, misspellings or slang, can be difficult. Additionally, cleaning and preprocessing the data (e.g., removing noise like unrelated tweets, abbreviations) is crucial but time-consuming.\n",
    "\n",
    "2. Sentiment Analysis Accuracy:\n",
    "Accurately classifying the sentiment of tweets can be challenging due to the informal language, sarcasm, mixed sentiments and local dialects often used on X/Twitter.\n",
    "\n",
    "3. Identifying Specific Issues:\n",
    "Extracting and categorizing specific issues (e.g power outages, billing issues) mentioned in tweets can be complex due to the diverse ways in which customers describe their problems.\n",
    "\n",
    "4. Real-time Data Processing:\n",
    "Processing a continuous stream of tweets in real-time to provide timely insights and responses is demanding in terms of computational resources and model efficiency.\n",
    "\n",
    "5. Handling Multilingual and Local Dialects:\n",
    "Tweets may be in multiple languages or include local dialects, which can complicate sentiment analysis and issue detection. \n",
    "6. Evaluating Model Performance:\n",
    "Ensuring the models perform well across different contexts, languages, and over time requires ongoing evaluation and tuning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Proposed Solution\n",
    "\n",
    "* Use advanced Natural Language Processing (NLP) techniques and APIs (e.g., Twitter API) to collect and preprocess tweets.\n",
    "\n",
    "* Implement data cleaning scripts to filter out irrelevant data and normalize the text for consistent analysis. \n",
    "\n",
    "* Train sentiment analysis models using machine learning techniques such as supervised learning with labeled datasets\n",
    "\n",
    "* Implement a robust pipeline using tools for real-time data streaming and processing. Integrate with scalable cloud services such as AWS or Google Cloud to ensure the system can handle large volumes of data efficiently.\n",
    "\n",
    "* Utilize existing chatbot frameworks like Rasa, integrated with the sentiment analysis and issue categorization models. This chatbot should be able to provide relevant responses based on the sentiment and identified issues and direct users to appropriate resources or support channels.\n",
    "\n",
    "* Incorporate multilingual NLP models and fine-tune them with local dialect data. Using translation APIs where necessary to standardize inputs before analysis.\n",
    "\n",
    "* Set up a continuous evaluation framework using A/B testing, cross-validation and performance metrics such as accuracy, F1-score and precision/recall. Regularly retrain models with new data to adapt to evolving customer language and sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Metrics of success:\n",
    "\n",
    "* Sentiment Accuracy: Percentage of correctly classified sentiments (positive, negative, neutral).\n",
    "\n",
    "* Issue Detection Rate: Number of key issues identified and addressed based on sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The analysis of the tweets reveals that for Kenya Power and Lightning Company(KPLC),sentiment analysis of the tweets can o along way in assisting the company to understand and deal with customer feedback.In this way,KPLC will be able to focus on identifying the main problems developing and implementing corresponding strategies for the company‚Äôs service improvement and ultimately increasing the customer satisfaction level of their customers .The company will be able to maintain their brand image and identify the impending issues before they happen.\n",
    "\n",
    "Despite the difficulties like dealing with  vast data and identification while analyzing the social media concerns ,performing sentiment analysis by analyzing tweets is effective.Since KLC has established key performance indicators of some of its goals such as raise in customer satisfaction scores and positive trend on brand sentiment,the company can use this tool to sustain its leadership in the energy sector while at the same time strengthening its relations with customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary Modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all CSV files into one CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 4 files into Data.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory containing the CSV files\n",
    "csv_directory = r'C:\\Users\\USER\\Desktop\\Data\\PHASE5\\KPLC'\n",
    "\n",
    "# Specify the output file name\n",
    "output_file = 'Data.csv'\n",
    "\n",
    "# Create an empty list to hold DataFrames\n",
    "csv_list = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory, filename)\n",
    "        try:\n",
    "            # Try reading the CSV file with UTF-8 encoding\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # If UTF-8 fails, try with ISO-8859-1 encoding\n",
    "            df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        csv_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "merged_df = pd.concat(csv_list, ignore_index=True)\n",
    "\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'Merged {len(csv_list)} files into {output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the basic structure of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_Loader class loads data\n",
    "class Data_Loader:\n",
    "    def __init__(self, data=None):\n",
    "        self.data = data\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        # If data is not already loaded, load it\n",
    "        if self.data is None:\n",
    "            self.data = pd.read_csv(file_path)\n",
    "        return self.data\n",
    "\n",
    "# load data from a file\n",
    "df = loader.load_data(r'C:\\Users\\USER\\Desktop\\Data\\PHASE5\\KPLC\\Data.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us look at the info, shape and data types that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the dataset:\n",
      "(45190, 9)\n",
      "\n",
      "Information about the Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45190 entries, 0 to 45189\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Author        45105 non-null  object \n",
      " 1   Handle        45190 non-null  object \n",
      " 2   Post          45185 non-null  object \n",
      " 3   Date          45190 non-null  object \n",
      " 4   Likes         7930 non-null   object \n",
      " 5   Reposts       1005 non-null   object \n",
      " 6   Comments      4595 non-null   float64\n",
      " 7   Post Link     8115 non-null   object \n",
      " 8   Profile Lƒ∞nk  8115 non-null   object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 3.1+ MB\n",
      "None\n",
      "\n",
      "Columns and their data types:\n",
      "Author: object\n",
      "Handle: object\n",
      "Post: object\n",
      "Date: object\n",
      "Likes: object\n",
      "Reposts: object\n",
      "Comments: float64\n",
      "Post Link: object\n",
      "Profile Lƒ∞nk: object\n",
      "              Author           Handle  \\\n",
      "0            ùíüùìéùìÉùí∂ùìàùìâùìé   @dynastyycolee   \n",
      "1  Abhishek ùêÅùêöùêúùê°ùêúùê°ùêöùêß  @juniorbachchan   \n",
      "2     YabaLeftOnline  @yabaleftonline   \n",
      "3     YabaLeftOnline  @yabaleftonline   \n",
      "4     YabaLeftOnline  @yabaleftonline   \n",
      "\n",
      "                                                Post Date Likes Reposts  \\\n",
      "0  Update: We just graduated nursing school toget...  21h   41K    6.6K   \n",
      "1  Represent!!\\n#JaiHind #ComeonIndia\\n\\n@omegawa...  23h   12K     315   \n",
      "2  Who is going to remind Cucurella that he‚Äôs a C...  20h   10K    2.3K   \n",
      "3  Asake needs to understand that his fans are no...   6h  6.7K     472   \n",
      "4                 Rate Asake‚Äôs new album from 1 - 10  10h  6.4K     417   \n",
      "\n",
      "   Comments                     Post Link                  Profile Lƒ∞nk  \n",
      "0     546.0   https://x.com/dynastyycolee   https://x.com/dynastyycolee  \n",
      "1     206.0  https://x.com/juniorbachchan  https://x.com/juniorbachchan  \n",
      "2     513.0  https://x.com/yabaleftonline  https://x.com/yabaleftonline  \n",
      "3     723.0  https://x.com/yabaleftonline  https://x.com/yabaleftonline  \n",
      "4     893.0  https://x.com/yabaleftonline  https://x.com/yabaleftonline  \n"
     ]
    }
   ],
   "source": [
    "class Data_Loader:\n",
    "    def load_data(self, file_path):\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "class Data_Informer(Data_Loader):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def print_info(self, df):\n",
    "        # Shape of the dataframe\n",
    "        print(\"\\nShape of the dataset:\")\n",
    "        print(df.shape)\n",
    "        \n",
    "        # Column data Information\n",
    "        print(\"\\nInformation about the Dataset:\")\n",
    "        print(df.info())\n",
    "        \n",
    "        # Data Types\n",
    "        data_types = df.dtypes\n",
    "        print(\"\\nColumns and their data types:\")\n",
    "        for column, dtype in data_types.items():\n",
    "            print(f\"{column}: {dtype}\")\n",
    "\n",
    "# Create an instance of Data_Informer\n",
    "data_informer = Data_Informer()\n",
    "\n",
    "# Load the dataset using the instance\n",
    "data = data_informer.load_data(r\"C:\\Users\\USER\\Desktop\\Data\\PHASE5\\KPLC\\Data.csv\")\n",
    "\n",
    "# Call the print_info method to print information about the dataset\n",
    "data_informer.print_info(data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has 9 features and about 45000 entries with different data types such us objects(8) and integers(1).\n",
    "\n",
    "\n",
    "Now we can proceed to data cleaning by removing duplicate values, Outliers and fill in any missing value using mean, median and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45190 entries, 0 to 45189\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Author        45105 non-null  object \n",
      " 1   Handle        45190 non-null  object \n",
      " 2   Post          45185 non-null  object \n",
      " 3   Date          45190 non-null  object \n",
      " 4   Likes         7930 non-null   object \n",
      " 5   Reposts       1005 non-null   object \n",
      " 6   Comments      4595 non-null   float64\n",
      " 7   Post Link     8115 non-null   object \n",
      " 8   Profile Lƒ∞nk  8115 non-null   object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 37175\n",
      "Duplicates removed.\n",
      "                  Author        Handle  \\\n",
      "77  gilbert kaunda stima    @SuufStima   \n",
      "78    Sack of cool vibes   @79patrickm   \n",
      "79                 Njeru  @NjeruSamuel   \n",
      "80                 Njeru  @NjeruSamuel   \n",
      "81   JUZTUZ K Wa ARSENAL       @juztuz   \n",
      "\n",
      "                                                 Post          Date Likes  \\\n",
      "77             @KenyaPower what is prepay bill number  May 31, 2016   NaN   \n",
      "78  @kenyapower @kenyapower_care power outage Gwa-...  Apr 15, 2016   NaN   \n",
      "79  @KenyaPower @KenyaPower_Care  HAKUNA STIMA KIT...  May 29, 2016   NaN   \n",
      "80  rudisha Stima Kitale bwana, on! Off the whole ...  Jun 24, 2016   NaN   \n",
      "81  @KenyaPower you people mean that machakos yote...  Jul 11, 2016   NaN   \n",
      "\n",
      "   Reposts  Comments                  Post Link               Profile Lƒ∞nk  \n",
      "77     NaN       1.0    https://x.com/SuufStima    https://x.com/SuufStima  \n",
      "78     NaN       1.0   https://x.com/79patrickm   https://x.com/79patrickm  \n",
      "79     NaN       1.0  https://x.com/NjeruSamuel  https://x.com/NjeruSamuel  \n",
      "80     NaN       1.0  https://x.com/NjeruSamuel  https://x.com/NjeruSamuel  \n",
      "81     NaN       1.0       https://x.com/juztuz       https://x.com/juztuz  \n"
     ]
    }
   ],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def remove_outliers(self):\n",
    "        numeric_columns = self.data.select_dtypes(include=['number']).columns\n",
    "        for column in numeric_columns:\n",
    "            Q1 = self.data[column].quantile(0.25)\n",
    "            Q3 = self.data[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            self.data = self.data[~((self.data[column] < lower_bound) | (self.data[column] > upper_bound))]\n",
    "        return self.data\n",
    "    \n",
    "    def fill_missing_values(self, strategy='mean'):\n",
    "        if strategy == 'mean':\n",
    "            self.data = self.data.fillna(self.data.mean())\n",
    "        elif strategy == 'median':\n",
    "            self.data = self.data.fillna(self.data.median())\n",
    "        elif strategy == 'mode':\n",
    "            self.data = self.data.fillna(self.data.mode().iloc[0])\n",
    "        return self.data\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        # Check for duplicates\n",
    "        duplicate_rows = self.data[self.data.duplicated()]\n",
    "        print(f\"Number of duplicate rows: {len(duplicate_rows)}\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        self.data = self.data.drop_duplicates()\n",
    "        print(\"Duplicates removed.\")\n",
    "        return self.data\n",
    "\n",
    "# Assuming `data` is your DataFrame already loaded using Data_Informer\n",
    "data = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\Data\\PHASE5\\KPLC\\Data.csv\")  # Or loaded using Data_Informer\n",
    "\n",
    "# Create an instance of DataCleaner with the loaded data\n",
    "data_cleaner = DataCleaner(data)\n",
    "\n",
    "# Remove duplicates\n",
    "data_cleaner.remove_duplicates()\n",
    "\n",
    "# Remove outliers from all numeric columns\n",
    "data_cleaner.remove_outliers()\n",
    "\n",
    "# Fill missing values using the specified strategy (default is 'mean')\n",
    "data_cleaner.fill_missing_values(strategy='mean')\n",
    "\n",
    "# Now, `data_cleaner.data` holds the cleaned data\n",
    "print(data_cleaner.data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset had alot of duplicate value(37,175) and the best option was to fill the missing values to avoid losing too much of the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
